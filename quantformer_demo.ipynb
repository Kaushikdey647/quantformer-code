{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Quantformer: Transformer-based Quantitative Trading\n",
        "\n",
        "This notebook demonstrates the implementation of the Quantformer model as described in the paper:\n",
        "\"Quantformer: from attention to profit with a quantitative transformer trading strategy\"\n",
        "\n",
        "## Overview\n",
        "\n",
        "The Quantformer adapts the transformer architecture for quantitative trading by:\n",
        "1. Replacing word embeddings with linear embeddings for numerical data\n",
        "2. Removing positional encoding (time series have inherent order)\n",
        "3. Simplifying the decoder for classification tasks\n",
        "4. Using market sentiment information (returns and turnover rates)\n",
        "\n",
        "## Contents\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Data Generation and Preprocessing](#data)\n",
        "3. [Model Architecture](#model)\n",
        "4. [Training](#training)\n",
        "5. [Trading Strategy](#strategy)\n",
        "6. [Backtesting](#backtesting)\n",
        "7. [Results Analysis](#results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install torch numpy pandas scikit-learn matplotlib seaborn plotly tqdm wandb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Import our Quantformer modules\n",
        "from quantformer import (\n",
        "    Quantformer, QuantformerTrainer, create_quantformer_model,\n",
        "    StockDataProcessor, prepare_training_data, create_sample_data,\n",
        "    QuantformerTradingStrategy, TradingConfig,\n",
        "    setup_training_experiment, setup_backtesting_experiment\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## WandB Setup (Optional)\n",
        "\n",
        "# Set up Weights & Biases for experiment tracking\n",
        "# You can disable this by setting ENABLE_WANDB = False\n",
        "ENABLE_WANDB = True  # Set to False to disable WandB logging\n",
        "\n",
        "if ENABLE_WANDB:\n",
        "    try:\n",
        "        import wandb\n",
        "        print(\"üîß WandB is available for experiment tracking\")\n",
        "        print(\"üìù To use WandB:\")\n",
        "        print(\"   1. Create a free account at https://wandb.ai\")\n",
        "        print(\"   2. Run 'wandb login' in terminal\")\n",
        "        print(\"   3. Or set WANDB_API_KEY environment variable\")\n",
        "        print(\"   4. The notebook will automatically log experiments\")\n",
        "        \n",
        "        # Test WandB connection (optional)\n",
        "        # Uncomment the next line to test WandB setup\n",
        "        # wandb.login()\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  WandB not installed. Install with: pip install wandb\")\n",
        "        ENABLE_WANDB = False\n",
        "else:\n",
        "    print(\"üìä WandB logging disabled - experiments will run without tracking\")\n",
        "\n",
        "print(f\"WandB Status: {'Enabled' if ENABLE_WANDB else 'Disabled'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "N_STOCKS = 200  # Number of stocks in our universe\n",
        "N_TIMESTEPS = 1000  # Total time periods\n",
        "SEQ_LEN = 20  # Input sequence length (as in paper)\n",
        "N_CLASSES = 3  # Number of quantile classes (œÅ in paper)\n",
        "PHI = 0.2  # Percentage of stocks for each quantile\n",
        "\n",
        "print(f\"Generating synthetic stock data...\")\n",
        "print(f\"- {N_STOCKS} stocks\")\n",
        "print(f\"- {N_TIMESTEPS} time periods\")\n",
        "print(f\"- {SEQ_LEN}-day input sequences\")\n",
        "\n",
        "# Generate sample data with improved realism\n",
        "feature_data, return_data = create_sample_data(\n",
        "    n_stocks=N_STOCKS, \n",
        "    n_timesteps=N_TIMESTEPS, \n",
        "    seq_len=SEQ_LEN,\n",
        "    random_seed=123  # Different seed for more challenging data\n",
        ")\n",
        "\n",
        "print(f\"Feature data shape: {feature_data.shape}\")\n",
        "print(f\"Return data shape: {return_data.shape}\")\n",
        "print(f\"Features: [returns, turnover_rates]\")\n",
        "\n",
        "# Check data statistics\n",
        "print(f\"\\nData Statistics:\")\n",
        "print(f\"Returns - Mean: {return_data.mean():.6f}, Std: {return_data.std():.6f}\")\n",
        "print(f\"Returns - Range: [{return_data.min():.6f}, {return_data.max():.6f}]\")\n",
        "print(f\"Turnover - Mean: {feature_data[:,:,1].mean():.6f}, Std: {feature_data[:,:,1].std():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration (as specified in the paper)\n",
        "model_config = {\n",
        "    'input_dim': 2,  # Returns and turnover rates\n",
        "    'd_model': 32,   # Increased hidden dimension for better capacity\n",
        "    'n_heads': 8,    # Number of attention heads\n",
        "    'n_layers': 4,   # Reduced layers to prevent overfitting\n",
        "    'd_ff': 128,     # Increased feed-forward dimension\n",
        "    'n_classes': N_CLASSES,\n",
        "    'seq_len': SEQ_LEN,\n",
        "    'dropout': 0.2   # Increased dropout for regularization\n",
        "}\n",
        "\n",
        "# Create model\n",
        "model = create_quantformer_model(model_config)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "print(f\"Model configuration: {model_config}\")\n",
        "\n",
        "# Test model with sample input to verify it works\n",
        "sample_input = torch.randn(1, SEQ_LEN, 2).to(device)\n",
        "with torch.no_grad():\n",
        "    sample_output = model(sample_input)\n",
        "    print(f\"Sample output shape: {sample_output.shape}\")\n",
        "    print(f\"Sample output probabilities: {sample_output.cpu().numpy().flatten()}\")\n",
        "    print(f\"Sum of probabilities: {sample_output.sum().item():.6f} (should be ~1.0)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training and validation data\n",
        "print(\"Preparing training data...\")\n",
        "\n",
        "train_loader, val_loader, processor = prepare_training_data(\n",
        "    feature_data=feature_data,\n",
        "    return_data=return_data,\n",
        "    seq_len=SEQ_LEN,\n",
        "    n_classes=N_CLASSES,\n",
        "    phi=PHI,\n",
        "    train_ratio=0.8,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "\n",
        "# Examine a batch\n",
        "for batch_x, batch_y in train_loader:\n",
        "    print(f\"Batch input shape: {batch_x.shape}\")\n",
        "    print(f\"Batch label shape: {batch_y.shape}\")\n",
        "    print(f\"Sample label distribution: {batch_y.sum(dim=0)}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "EPOCHS = 25  # Slightly more epochs\n",
        "LEARNING_RATE = 0.0005  # Lower learning rate for stability\n",
        "\n",
        "# Set up WandB experiment tracking\n",
        "wandb_logger = None\n",
        "if ENABLE_WANDB:\n",
        "    # Create experiment configuration\n",
        "    training_config = {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"batch_size\": 64,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"loss_function\": \"MSE\"\n",
        "    }\n",
        "    \n",
        "    data_config = {\n",
        "        \"n_stocks\": N_STOCKS,\n",
        "        \"n_timesteps\": N_TIMESTEPS,\n",
        "        \"seq_len\": SEQ_LEN,\n",
        "        \"n_classes\": N_CLASSES,\n",
        "        \"phi\": PHI\n",
        "    }\n",
        "    \n",
        "    # Initialize WandB logger\n",
        "    wandb_logger = setup_training_experiment(\n",
        "        model_config=model_config,\n",
        "        training_config=training_config,\n",
        "        data_config=data_config,\n",
        "        notes=\"Quantformer training with improved synthetic data and fixed label generation\"\n",
        "    )\n",
        "    \n",
        "    # Log data statistics\n",
        "    if wandb_logger.enabled:\n",
        "        wandb_logger.log_data_statistics(feature_data, return_data)\n",
        "\n",
        "# Initialize trainer with WandB logger\n",
        "trainer = QuantformerTrainer(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    wandb_logger=wandb_logger\n",
        ")\n",
        "\n",
        "print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "if wandb_logger and wandb_logger.enabled:\n",
        "    print(f\"üîó WandB tracking: {wandb_logger.run.url}\")\n",
        "\n",
        "# Training loop with progress tracking\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Progress bar\n",
        "pbar = tqdm(range(EPOCHS), desc=\"Training\")\n",
        "\n",
        "for epoch in pbar:\n",
        "    # Training\n",
        "    epoch_train_loss = 0.0\n",
        "    model.train()\n",
        "    \n",
        "    for batch_x, batch_y in train_loader:\n",
        "        loss = trainer.train_step(batch_x, batch_y)\n",
        "        epoch_train_loss += loss\n",
        "    \n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_accuracy = trainer.evaluate(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    \n",
        "    # Update progress bar\n",
        "    pbar.set_postfix({\n",
        "        'Train Loss': f'{avg_train_loss:.4f}',\n",
        "        'Val Loss': f'{val_loss:.4f}',\n",
        "        'Val Acc': f'{val_accuracy:.4f}'\n",
        "    })\n",
        "    \n",
        "    # Early stopping if accuracy is suspiciously high\n",
        "    if val_accuracy > 0.95 and epoch > 5:\n",
        "        print(f\"\\nWarning: Very high accuracy ({val_accuracy:.4f}) detected at epoch {epoch+1}\")\n",
        "        print(\"This might indicate data leakage or overfitting.\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"Final validation accuracy: {val_accuracies[-1]:.4f}\")\n",
        "\n",
        "# Check if training was successful\n",
        "if val_accuracies[-1] > 0.9:\n",
        "    print(\"‚ö†Ô∏è  Very high accuracy - check for data issues!\")\n",
        "elif val_accuracies[-1] < 0.4:\n",
        "    print(\"‚ö†Ô∏è  Very low accuracy - model might need tuning!\")\n",
        "else:\n",
        "    print(\"‚úÖ Training completed with reasonable accuracy!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves with detailed analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss curves\n",
        "axes[0, 0].plot(train_losses, label='Training Loss', linewidth=2, color='blue')\n",
        "axes[0, 0].plot(val_losses, label='Validation Loss', linewidth=2, color='red')\n",
        "axes[0, 0].set_title('Training and Validation Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_yscale('log')  # Log scale for better visualization\n",
        "\n",
        "# Accuracy curve\n",
        "axes[0, 1].plot(val_accuracies, label='Validation Accuracy', linewidth=2, color='green')\n",
        "axes[0, 1].axhline(y=0.33, color='gray', linestyle='--', alpha=0.7, label='Random Guess (33%)')\n",
        "axes[0, 1].set_title('Validation Accuracy')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_ylim([0, 1])\n",
        "\n",
        "# Loss difference (overfitting indicator)\n",
        "loss_diff = np.array(val_losses) - np.array(train_losses)\n",
        "axes[1, 0].plot(loss_diff, linewidth=2, color='purple')\n",
        "axes[1, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.7)\n",
        "axes[1, 0].set_title('Validation - Training Loss (Overfitting Indicator)')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss Difference')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning curve analysis\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "axes[1, 1].plot(epochs, train_losses, 'o-', label='Training', alpha=0.7)\n",
        "axes[1, 1].plot(epochs, val_losses, 's-', label='Validation', alpha=0.7)\n",
        "axes[1, 1].set_title('Learning Curves (Detailed)')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Training analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "final_train_loss = train_losses[-1]\n",
        "final_val_loss = val_losses[-1]\n",
        "final_accuracy = val_accuracies[-1]\n",
        "\n",
        "print(f\"Final Training Loss: {final_train_loss:.6f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.6f}\")\n",
        "print(f\"Loss Difference: {final_val_loss - final_train_loss:.6f}\")\n",
        "print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Check for common issues\n",
        "if final_accuracy > 0.95:\n",
        "    print(\"\\nüö® POTENTIAL ISSUES DETECTED:\")\n",
        "    print(\"- Accuracy too high (>95%) - possible data leakage\")\n",
        "    print(\"- Check if future information is leaking into features\")\n",
        "    print(\"- Verify label generation is correct\")\n",
        "elif final_accuracy < 0.4:\n",
        "    print(\"\\nüö® POTENTIAL ISSUES DETECTED:\")\n",
        "    print(\"- Accuracy too low (<40%) - model not learning\")\n",
        "    print(\"- Try increasing learning rate or model capacity\")\n",
        "    print(\"- Check if data preprocessing is correct\")\n",
        "elif abs(final_val_loss - final_train_loss) > 0.1:\n",
        "    print(\"\\n‚ö†Ô∏è  OVERFITTING DETECTED:\")\n",
        "    print(\"- Large gap between training and validation loss\")\n",
        "    print(\"- Consider increasing dropout or reducing model complexity\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ TRAINING LOOKS HEALTHY:\")\n",
        "    print(\"- Reasonable accuracy range\")\n",
        "    print(\"- No significant overfitting\")\n",
        "    print(\"- Model is learning properly\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log training results to WandB\n",
        "if wandb_logger and wandb_logger.enabled:\n",
        "    print(\"üìä Logging training results to WandB...\")\n",
        "    \n",
        "    # Log training curves\n",
        "    wandb_logger.log_training_curves(train_losses, val_losses, val_accuracies)\n",
        "    \n",
        "    # Log final metrics\n",
        "    wandb_logger.log_metrics({\n",
        "        \"final/train_loss\": train_losses[-1],\n",
        "        \"final/val_loss\": val_losses[-1], \n",
        "        \"final/val_accuracy\": val_accuracies[-1],\n",
        "        \"final/epochs_trained\": len(train_losses)\n",
        "    })\n",
        "    \n",
        "    # Save model artifact\n",
        "    wandb_logger.log_model_artifact(\n",
        "        model, \n",
        "        name=\"quantformer_trained_model\",\n",
        "        metadata={\n",
        "            \"final_train_loss\": train_losses[-1],\n",
        "            \"final_val_loss\": val_losses[-1],\n",
        "            \"final_val_accuracy\": val_accuracies[-1],\n",
        "            \"epochs_trained\": len(train_losses),\n",
        "            \"model_config\": model_config\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Training results logged to WandB\")\n",
        "else:\n",
        "    print(\"üìä WandB logging disabled - results not logged\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Fixes Applied for Training Issues\n",
        "\n",
        "If you previously saw perfect accuracy (1.0) and very low loss, here are the fixes that have been applied:\n",
        "\n",
        "### 1. **Fixed Quantile Label Generation**\n",
        "- Corrected the quantile range calculation in `create_quantile_labels()`\n",
        "- Now properly creates non-overlapping quantile ranges\n",
        "- For 3 classes: Bottom 20%, Middle 20%, Top 20% (with gaps as in paper)\n",
        "\n",
        "### 2. **Improved Synthetic Data Generation**\n",
        "- More realistic stock returns with varying volatility per stock\n",
        "- Added momentum and mean-reversion effects\n",
        "- Correlated turnover rates with volatility\n",
        "- Market-wide effects to simulate real market conditions\n",
        "- Different random seed to avoid overfitting to specific patterns\n",
        "\n",
        "### 3. **Model Architecture Adjustments**\n",
        "- Increased model capacity (d_model=32, d_ff=128)\n",
        "- Reduced layers to prevent overfitting (n_layers=4)\n",
        "- Increased dropout for regularization (0.2)\n",
        "- Lower learning rate for stability (0.0005)\n",
        "\n",
        "### 4. **Enhanced Debugging**\n",
        "- Added data statistics and distribution checks\n",
        "- Training progress monitoring with early warning systems\n",
        "- Detailed analysis of potential overfitting or underfitting\n",
        "\n",
        "### Expected Results\n",
        "With these fixes, you should now see:\n",
        "- **Accuracy**: 50-80% (reasonable for financial prediction)\n",
        "- **Loss**: Gradually decreasing but not reaching zero\n",
        "- **Learning**: Steady improvement without perfect predictions\n",
        "\n",
        "The previous perfect accuracy was likely due to data leakage or overly simplistic synthetic data patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trading strategy configuration\n",
        "trading_config = TradingConfig(\n",
        "    n_classes=N_CLASSES,\n",
        "    decision_factor=1,  # Select top 1 quantile (b=1 in paper)\n",
        "    phi=PHI,\n",
        "    transaction_fee=0.003,  # 0.3% as in paper\n",
        "    initial_capital=1000000.0,  # $1M initial capital\n",
        "    rebalance_frequency='monthly'\n",
        ")\n",
        "\n",
        "print(f\"Trading Strategy Configuration:\")\n",
        "print(f\"- Number of classes: {trading_config.n_classes}\")\n",
        "print(f\"- Decision factor: {trading_config.decision_factor}\")\n",
        "print(f\"- Phi (quantile size): {trading_config.phi}\")\n",
        "print(f\"- Transaction fee: {trading_config.transaction_fee:.1%}\")\n",
        "print(f\"- Initial capital: ${trading_config.initial_capital:,.0f}\")\n",
        "\n",
        "# Set up WandB for backtesting (if enabled)\n",
        "backtest_logger = None\n",
        "if ENABLE_WANDB:\n",
        "    strategy_config_dict = {\n",
        "        \"n_classes\": trading_config.n_classes,\n",
        "        \"decision_factor\": trading_config.decision_factor,\n",
        "        \"phi\": trading_config.phi,\n",
        "        \"transaction_fee\": trading_config.transaction_fee,\n",
        "        \"initial_capital\": trading_config.initial_capital,\n",
        "        \"rebalance_frequency\": trading_config.rebalance_frequency\n",
        "    }\n",
        "    \n",
        "    backtest_logger = setup_backtesting_experiment(\n",
        "        strategy_config=strategy_config_dict,\n",
        "        notes=\"Quantformer backtesting with trained model\"\n",
        "    )\n",
        "\n",
        "# Initialize trading strategy with WandB logger\n",
        "strategy = QuantformerTradingStrategy(model, trading_config, wandb_logger=backtest_logger)\n",
        "print(f\"\\nTrading strategy initialized\")\n",
        "if backtest_logger and backtest_logger.enabled:\n",
        "    print(f\"üîó Backtesting WandB: {backtest_logger.run.url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for backtesting\n",
        "backtest_start = int(0.8 * N_TIMESTEPS)  # Start backtesting from 80% of the data\n",
        "backtest_periods = min(100, N_TIMESTEPS - backtest_start - SEQ_LEN)  # Limit for demo\n",
        "\n",
        "print(f\"Preparing backtest data...\")\n",
        "print(f\"Backtest start: timestep {backtest_start}\")\n",
        "print(f\"Backtest periods: {backtest_periods}\")\n",
        "\n",
        "# Create features for backtesting\n",
        "features_for_backtest = np.zeros((backtest_periods, N_STOCKS, SEQ_LEN, 2))\n",
        "returns_for_backtest = np.zeros((backtest_periods, N_STOCKS))\n",
        "\n",
        "for t in range(backtest_periods):\n",
        "    actual_t = backtest_start + t\n",
        "    \n",
        "    # Create sequences for each stock at this timestep\n",
        "    for stock in range(N_STOCKS):\n",
        "        features_for_backtest[t, stock] = feature_data[actual_t:actual_t + SEQ_LEN, stock]\n",
        "    \n",
        "    # Get returns for next period\n",
        "    if actual_t + SEQ_LEN < N_TIMESTEPS:\n",
        "        returns_for_backtest[t] = return_data[actual_t + SEQ_LEN]\n",
        "\n",
        "print(f\"Features for backtest shape: {features_for_backtest.shape}\")\n",
        "print(f\"Returns for backtest shape: {returns_for_backtest.shape}\")\n",
        "\n",
        "# Run backtest\n",
        "print(\"\\nRunning backtest...\")\n",
        "backtest_results = strategy.backtest(\n",
        "    features_data=features_for_backtest,\n",
        "    returns_data=returns_for_backtest\n",
        ")\n",
        "\n",
        "print(\"Backtest completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display performance metrics\n",
        "print(\"=\" * 50)\n",
        "print(\"QUANTFORMER TRADING STRATEGY RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for metric, value in backtest_results.items():\n",
        "    if isinstance(value, float):\n",
        "        if 'return' in metric.lower() or 'alpha' in metric.lower():\n",
        "            print(f\"{metric.replace('_', ' ').title():<25}: {value:>10.2%}\")\n",
        "        elif 'ratio' in metric.lower():\n",
        "            print(f\"{metric.replace('_', ' ').title():<25}: {value:>10.4f}\")\n",
        "        elif 'value' in metric.lower():\n",
        "            print(f\"{metric.replace('_', ' ').title():<25}: ${value:>10,.0f}\")\n",
        "        else:\n",
        "            print(f\"{metric.replace('_', ' ').title():<25}: {value:>10.4f}\")\n",
        "\n",
        "# Get portfolio history for visualization\n",
        "portfolio_df = strategy.get_portfolio_history()\n",
        "\n",
        "if not portfolio_df.empty:\n",
        "    print(f\"\\nPortfolio history shape: {portfolio_df.shape}\")\n",
        "    \n",
        "    # Create performance visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Portfolio value over time\n",
        "    axes[0, 0].plot(portfolio_df['portfolio_value'], linewidth=2, color='blue')\n",
        "    axes[0, 0].axhline(y=trading_config.initial_capital, color='red', linestyle='--', alpha=0.7, label='Initial Capital')\n",
        "    axes[0, 0].set_title('Portfolio Value Over Time')\n",
        "    axes[0, 0].set_xlabel('Time Period')\n",
        "    axes[0, 0].set_ylabel('Portfolio Value ($)')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Cumulative returns\n",
        "    axes[0, 1].plot(portfolio_df['cumulative_return'] * 100, linewidth=2, color='green')\n",
        "    axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
        "    axes[0, 1].set_title('Cumulative Returns')\n",
        "    axes[0, 1].set_xlabel('Time Period')\n",
        "    axes[0, 1].set_ylabel('Cumulative Return (%)')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Daily returns distribution\n",
        "    daily_returns = portfolio_df['returns'][1:] * 100  # Skip first zero return\n",
        "    axes[1, 0].hist(daily_returns, bins=20, alpha=0.7, edgecolor='black', color='purple')\n",
        "    axes[1, 0].axvline(x=daily_returns.mean(), color='red', linestyle='--', label=f'Mean: {daily_returns.mean():.3f}%')\n",
        "    axes[1, 0].set_title('Distribution of Daily Returns')\n",
        "    axes[1, 0].set_xlabel('Daily Return (%)')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Portfolio turnover over time\n",
        "    axes[1, 1].plot(portfolio_df['turnover'] * 100, linewidth=2, color='orange')\n",
        "    axes[1, 1].set_title('Portfolio Turnover Over Time')\n",
        "    axes[1, 1].set_xlabel('Time Period')\n",
        "    axes[1, 1].set_ylabel('Turnover Rate (%)')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No portfolio history available for visualization.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Finish WandB Runs\n",
        "\n",
        "# Clean up WandB runs\n",
        "if ENABLE_WANDB:\n",
        "    if wandb_logger and wandb_logger.enabled:\n",
        "        wandb_logger.finish()\n",
        "        print(\"‚úÖ Training WandB run finished\")\n",
        "    \n",
        "    if backtest_logger and backtest_logger.enabled:\n",
        "        backtest_logger.finish()\n",
        "        print(\"‚úÖ Backtesting WandB run finished\")\n",
        "    \n",
        "    print(\"üéâ All WandB experiments completed!\")\n",
        "    print(\"üìä Check your WandB dashboard for detailed results and comparisons\")\n",
        "else:\n",
        "    print(\"üìä WandB was disabled - no runs to finish\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
